1
00:00:00,180 --> 00:00:02,009
Now let's learn about a new service,

2
00:00:02,009 --> 00:00:04,110
which is Kinesis Data Firehose.

3
00:00:04,110 --> 00:00:06,510
So it is a very helpful service

4
00:00:06,510 --> 00:00:09,330
that can take data from producers.

5
00:00:09,330 --> 00:00:11,880
And producers can be everything we've seen

6
00:00:11,880 --> 00:00:15,120
for Kinesis Data Stream, so applications, clients, SDK,

7
00:00:15,120 --> 00:00:17,100
or the Kinesis agents can all produce

8
00:00:17,100 --> 00:00:19,050
into your Kinesis Data Firehose.

9
00:00:19,050 --> 00:00:21,240
But also, a Kinesis Data Stream

10
00:00:21,240 --> 00:00:23,550
can produce into Kinesis Sata Firehose.

11
00:00:23,550 --> 00:00:25,470
Amazon CloudWatch logs and events

12
00:00:25,470 --> 00:00:27,810
can produce into Kinesis Data Firehose.

13
00:00:27,810 --> 00:00:30,210
And all these applications are going to send records

14
00:00:30,210 --> 00:00:32,189
into Kinesis Data Firehose,

15
00:00:32,189 --> 00:00:34,800
and then Kinesis Data Firehose can optionally choose

16
00:00:34,800 --> 00:00:37,560
to transform the data using a Lambda function,

17
00:00:37,560 --> 00:00:38,970
but this is optional.

18
00:00:38,970 --> 00:00:41,460
And once the data is transformed optionally,

19
00:00:41,460 --> 00:00:45,390
then it can be written in batches into destinations.

20
00:00:45,390 --> 00:00:48,300
So Kinesis Data Firehose text data from sources.

21
00:00:48,300 --> 00:00:50,910
Usually the most common is going to be Kinesis Data Streams

22
00:00:50,910 --> 00:00:54,780
and it's going to write this data into destinations

23
00:00:54,780 --> 00:00:56,460
without you writing any kind of code,

24
00:00:56,460 --> 00:01:00,480
because Kinesis Data Firehose knows how to write data.

25
00:01:00,480 --> 00:01:02,340
So there are three kinds of destinations

26
00:01:02,340 --> 00:01:03,960
with Kinesis Data Firehose.

27
00:01:03,960 --> 00:01:07,170
The number one category is AWS destinations

28
00:01:07,170 --> 00:01:09,330
and you need to know them by heart.

29
00:01:09,330 --> 00:01:11,010
So the first one is Amazon S3,

30
00:01:11,010 --> 00:01:14,430
so you can write all your data into Amazon S3.

31
00:01:14,430 --> 00:01:16,530
The second one is Amazon Redshift,

32
00:01:16,530 --> 00:01:18,630
which is a warehousing database.

33
00:01:18,630 --> 00:01:22,860
And to do so, it first writes the data into Amazon S3.

34
00:01:22,860 --> 00:01:25,860
And then Kinesis Data Firehose will issue a copy command,

35
00:01:25,860 --> 00:01:28,380
and this copy command is going to copy data

36
00:01:28,380 --> 00:01:31,920
from Amazon S3 into Amazon Redshift.

37
00:01:31,920 --> 00:01:36,900
And the last destination on AWS is called Amazon OpenSearch.

38
00:01:36,900 --> 00:01:40,230
There are also some third party partner destinations.

39
00:01:40,230 --> 00:01:43,350
So Kinesis Data Firehose can send data into Datadog,

40
00:01:43,350 --> 00:01:45,960
Splunk, New Relic, MongoDB.

41
00:01:45,960 --> 00:01:48,720
And this list can get bigger and bigger over time,

42
00:01:48,720 --> 00:01:51,480
so it will not update this if there are new partners.

43
00:01:51,480 --> 00:01:53,220
But just so you know, they are partners

44
00:01:53,220 --> 00:01:55,710
that Kinesis Data Firehose can send data to.

45
00:01:55,710 --> 00:02:00,360
Or finally, if you have your own API with an HTTP endpoint,

46
00:02:00,360 --> 00:02:03,210
it is for you to send data from Kinesis Data Firehose

47
00:02:03,210 --> 00:02:05,673
into a custom destination.

48
00:02:06,810 --> 00:02:09,960
Okay, so once the data is sent into all these destinations,

49
00:02:09,960 --> 00:02:10,793
you have two options.

50
00:02:10,793 --> 00:02:12,990
You can also send all the data

51
00:02:12,990 --> 00:02:16,410
into an S3 bucket as a backup

52
00:02:16,410 --> 00:02:18,420
or just send the data that was failed

53
00:02:18,420 --> 00:02:20,070
to be written into these destinations

54
00:02:20,070 --> 00:02:23,460
into a failed S3 buckets.

55
00:02:23,460 --> 00:02:25,470
So to summarize,

56
00:02:25,470 --> 00:02:27,990
Kinesis Data Firehose is a fully managed service,

57
00:02:27,990 --> 00:02:30,750
so there's no administration, automated scaling,

58
00:02:30,750 --> 00:02:33,030
and it is serverless, so no service to manage.

59
00:02:33,030 --> 00:02:35,880
You can send data into AWS destinations

60
00:02:35,880 --> 00:02:39,720
such as the Redshift, Amazon S3, and OpenSearch,

61
00:02:39,720 --> 00:02:42,540
third party partners such as Splunk, MongoDB,

62
00:02:42,540 --> 00:02:44,670
Datadog, new Relic, et cetera, et cetera,

63
00:02:44,670 --> 00:02:48,960
and custom destinations to any HTTP endpoints.

64
00:02:48,960 --> 00:02:50,700
You're going to pay only for the data

65
00:02:50,700 --> 00:02:52,200
going through Firehose,

66
00:02:52,200 --> 00:02:55,050
so this is a very good data pressing model,

67
00:02:55,050 --> 00:02:56,670
and it is a near real time.

68
00:02:56,670 --> 00:02:57,503
Why?

69
00:02:57,503 --> 00:02:59,940
Well, because we write data in batches

70
00:02:59,940 --> 00:03:01,950
from Firehose to the destination.

71
00:03:01,950 --> 00:03:04,470
So there's going to be a buffer interval.

72
00:03:04,470 --> 00:03:07,890
Either it's zero seconds and you've disabled buffering

73
00:03:07,890 --> 00:03:10,830
or you set it to a higher number and then you have buffering

74
00:03:10,830 --> 00:03:13,680
and you can set it up up to 900 seconds.

75
00:03:13,680 --> 00:03:15,180
And then if you have buffering,

76
00:03:15,180 --> 00:03:17,400
you should also specify a buffer size.

77
00:03:17,400 --> 00:03:18,960
So it's minimum one megabyte

78
00:03:18,960 --> 00:03:20,700
and you can go to higher number.

79
00:03:20,700 --> 00:03:22,590
So in case you have buffering,

80
00:03:22,590 --> 00:03:24,360
well, that makes Kinesis Data Firehose

81
00:03:24,360 --> 00:03:26,220
a near real-time service.

82
00:03:26,220 --> 00:03:28,410
And if you don't have buffering,

83
00:03:28,410 --> 00:03:30,720
if you have zero seconds for your buffer interval,

84
00:03:30,720 --> 00:03:32,880
it's still going to be considered near real-time

85
00:03:32,880 --> 00:03:35,250
because going to take a few seconds

86
00:03:35,250 --> 00:03:39,120
to deliver your data into your destination.

87
00:03:39,120 --> 00:03:41,580
It supports many data formats, conversions,

88
00:03:41,580 --> 00:03:43,230
transformation, and compressions,

89
00:03:43,230 --> 00:03:45,480
and you can write your own data transformation

90
00:03:45,480 --> 00:03:47,160
using Lambda if you needed to.

91
00:03:47,160 --> 00:03:49,860
Finally, you can send all the failed

92
00:03:49,860 --> 00:03:53,220
or all the data into a backup S3 buckets.

93
00:03:53,220 --> 00:03:55,440
So a question that comes up at the exam

94
00:03:55,440 --> 00:03:57,480
usually is to understand the difference

95
00:03:57,480 --> 00:03:59,520
of when to use Kinesis Data Streams

96
00:03:59,520 --> 00:04:01,320
and Kinesis Data Firehose.

97
00:04:01,320 --> 00:04:04,230
So should be very easy for you now if you followed closely,

98
00:04:04,230 --> 00:04:05,340
but let's summarize.

99
00:04:05,340 --> 00:04:07,560
Kinesis Data Streams is just streaming service

100
00:04:07,560 --> 00:04:09,930
used to ingest data at scale

101
00:04:09,930 --> 00:04:11,190
and you write your own custom code

102
00:04:11,190 --> 00:04:12,930
for your producers and your consumers.

103
00:04:12,930 --> 00:04:16,410
It's real time, so 200 millisecond or 70 millisecond,

104
00:04:16,410 --> 00:04:17,730
and you manage scaling yourself,

105
00:04:17,730 --> 00:04:19,440
you do chart splitting and chart merging

106
00:04:19,440 --> 00:04:21,570
to increase the scale and throughputs.

107
00:04:21,570 --> 00:04:22,800
You're going to also pay

108
00:04:22,800 --> 00:04:25,440
for how much capacity you have provision.

109
00:04:25,440 --> 00:04:27,390
The data storage in the Kinesis Data Stream

110
00:04:27,390 --> 00:04:30,660
can be between one to 365 days.

111
00:04:30,660 --> 00:04:33,300
This allows multiple consumers to read from the same stream

112
00:04:33,300 --> 00:04:36,330
and also supports replay capability.

113
00:04:36,330 --> 00:04:39,420
Whereas Kinesis Data Firehose is an ingestion service

114
00:04:39,420 --> 00:04:42,810
to stream data into S3, Redshift, OpenSearch,

115
00:04:42,810 --> 00:04:45,150
which a third party or a custom HTTP.

116
00:04:45,150 --> 00:04:47,070
It is fully managed, no service to manage.

117
00:04:47,070 --> 00:04:48,030
It is near real time.

118
00:04:48,030 --> 00:04:49,920
So remember, this near real-time

119
00:04:49,920 --> 00:04:53,760
is a keyword you need to look at in your exam questions.

120
00:04:53,760 --> 00:04:54,840
There is automated scaling

121
00:04:54,840 --> 00:04:56,550
so no need for you to worry about it.

122
00:04:56,550 --> 00:04:57,570
And you're going to pay only

123
00:04:57,570 --> 00:05:00,120
for what goes through Kinesis Data Firehose.

124
00:05:00,120 --> 00:05:01,230
There is no data storage,

125
00:05:01,230 --> 00:05:04,500
so you cannot replay data from Kinesis Data Firehose.

126
00:05:04,500 --> 00:05:07,020
So yeah, it doesn't support the replay capability.

127
00:05:07,020 --> 00:05:11,550
So that's it for the overview of Kinesis Data Firehose.

128
00:05:11,550 --> 00:05:12,960
I hope that makes sense

129
00:05:12,960 --> 00:05:14,910
and I will see you in the next lecture.


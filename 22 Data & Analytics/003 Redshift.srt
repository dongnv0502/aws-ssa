1
00:00:00,030 --> 00:00:02,070
Okay, so now let's talk about Redshift.

2
00:00:02,070 --> 00:00:05,970
So Redshift is a database that's based on PostgreSQL,

3
00:00:05,970 --> 00:00:08,490
but it's not used for online transaction processing.

4
00:00:08,490 --> 00:00:13,050
Instead it's OLAP. So it's online analytical processing.

5
00:00:13,050 --> 00:00:16,530
So it's used for analytics and data warehousing.

6
00:00:16,530 --> 00:00:18,930
So any type of computations on your data,

7
00:00:18,930 --> 00:00:20,850
Redshift is a good place to do it.

8
00:00:20,850 --> 00:00:23,070
So it gives you 10 times better performance

9
00:00:23,070 --> 00:00:25,620
than other data warehouses, and it has big scale.

10
00:00:25,620 --> 00:00:27,960
You can scale to petabytes of data.

11
00:00:27,960 --> 00:00:32,009
The data is going to be stored as a column instead of a row.

12
00:00:32,009 --> 00:00:34,350
And so that's what makes it so good at analytics

13
00:00:34,350 --> 00:00:37,950
because it's able to quickly add all the data

14
00:00:37,950 --> 00:00:39,540
in a specific column.

15
00:00:39,540 --> 00:00:42,030
And on top of it, it has a parallel query engine

16
00:00:42,030 --> 00:00:44,310
to speed up any queries you may have.

17
00:00:44,310 --> 00:00:46,890
You have two modes of starting a Redshift cluster.

18
00:00:46,890 --> 00:00:50,700
You can do a provision cluster or a serverless cluster.

19
00:00:50,700 --> 00:00:54,450
You also use a SQL interface for performing the queries.

20
00:00:54,450 --> 00:00:57,390
And you have BI tools such as Amazon Quicksight

21
00:00:57,390 --> 00:01:00,720
or Tableau that have direct integration with it.

22
00:01:00,720 --> 00:01:03,810
So if you were to compare Redshift versus Athena, well,

23
00:01:03,810 --> 00:01:05,820
Redshift has faster queries, faster joins

24
00:01:05,820 --> 00:01:07,890
and aggregations thanks to indexes,

25
00:01:07,890 --> 00:01:10,620
but you need to of course provision a whole cluster,

26
00:01:10,620 --> 00:01:12,480
whereas Athena is serverless only

27
00:01:12,480 --> 00:01:15,180
and all your data lives in S3.

28
00:01:15,180 --> 00:01:16,830
So talking about this Redshift cluster,

29
00:01:16,830 --> 00:01:20,850
so you have the concept of a leader node and a compute node.

30
00:01:20,850 --> 00:01:23,210
So the leader node is for query planning

31
00:01:23,210 --> 00:01:25,470
and results aggregation,

32
00:01:25,470 --> 00:01:27,660
whereas the compute nodes in the Redshift cluster is going

33
00:01:27,660 --> 00:01:29,880
to be to perform actually the queries

34
00:01:29,880 --> 00:01:32,280
and to send the results to the leader.

35
00:01:32,280 --> 00:01:35,220
When you are in provision mode, you have access

36
00:01:35,220 --> 00:01:37,740
to choosing the instant types in advance,

37
00:01:37,740 --> 00:01:40,080
and you can reserve instances if you wanted

38
00:01:40,080 --> 00:01:41,700
to do cost savings.

39
00:01:41,700 --> 00:01:43,890
But if you are in serverless mode,

40
00:01:43,890 --> 00:01:47,490
you just leave everything in the hands of AWS

41
00:01:47,490 --> 00:01:49,890
and you don't manage any type of cluster or node.

42
00:01:49,890 --> 00:01:53,314
So list or summarize it, the leader node has access

43
00:01:53,314 --> 00:01:54,570
to the compute nodes

44
00:01:54,570 --> 00:01:57,360
and it makes it a whole Redshift cluster.

45
00:01:57,360 --> 00:01:58,950
And so when you send a query, for example,

46
00:01:58,950 --> 00:02:01,950
select count from My_TABLE GROUP BY by na na na.

47
00:02:01,950 --> 00:02:04,680
The leader node is going to plan the query, send it

48
00:02:04,680 --> 00:02:07,500
to the compute nodes, you're going to execute the query

49
00:02:07,500 --> 00:02:10,080
and then send back the results to the leader node.

50
00:02:10,080 --> 00:02:12,660
And that's the architecture of Redshift.

51
00:02:12,660 --> 00:02:15,120
So let's talk about Snapshots and DR.

52
00:02:15,120 --> 00:02:17,940
So Redshift is single AZ for most clusters,

53
00:02:17,940 --> 00:02:21,570
but now there's a multi AZ mode for some cluster types.

54
00:02:21,570 --> 00:02:23,310
And so if you have multi AZ, then you're good

55
00:02:23,310 --> 00:02:24,660
for disaster recovery.

56
00:02:24,660 --> 00:02:27,240
But if you have single AZ, then to do disaster recovery,

57
00:02:27,240 --> 00:02:28,710
you need to use snapshots.

58
00:02:28,710 --> 00:02:31,320
So snapshots are point in time backups of a cluster,

59
00:02:31,320 --> 00:02:34,350
and they will be stored internally in Amazon S3,

60
00:02:34,350 --> 00:02:35,850
and they are incremental only

61
00:02:35,850 --> 00:02:37,770
where that has changed will be saved,

62
00:02:37,770 --> 00:02:39,930
and that will save you a lot of space, obviously.

63
00:02:39,930 --> 00:02:43,050
You can restore a snapshot into a new Redshift cluster,

64
00:02:43,050 --> 00:02:45,900
and you have two modes for snapshots.

65
00:02:45,900 --> 00:02:47,033
You can take them manually

66
00:02:47,033 --> 00:02:49,860
or you can take them in an automated way.

67
00:02:49,860 --> 00:02:51,780
So if you automate it, every eight hours is going

68
00:02:51,780 --> 00:02:56,220
to be a snapshot or every five gigabytes on the schedule.

69
00:02:56,220 --> 00:02:57,360
And then you can set the retention

70
00:02:57,360 --> 00:02:58,800
for your automated snapshots,

71
00:02:58,800 --> 00:03:00,810
or you wanna do a manual snapshot.

72
00:03:00,810 --> 00:03:02,670
The snapshot is going to be retained

73
00:03:02,670 --> 00:03:04,860
until you delete it manually.

74
00:03:04,860 --> 00:03:07,170
And then a really, really cool feature of Redshift is

75
00:03:07,170 --> 00:03:08,340
that you can configure Redshift

76
00:03:08,340 --> 00:03:11,040
to automatically copy snapshots, whether they're automated

77
00:03:11,040 --> 00:03:15,150
or manual of the cluster into another AWS region,

78
00:03:15,150 --> 00:03:17,700
hence giving you a disaster recovery strategy.

79
00:03:17,700 --> 00:03:20,250
So let's take this Redshift cluster, your original cluster,

80
00:03:20,250 --> 00:03:21,690
and then we have another region.

81
00:03:21,690 --> 00:03:23,220
So we're going to take snapshots.

82
00:03:23,220 --> 00:03:25,110
They're going to be automatically copied

83
00:03:25,110 --> 00:03:26,910
into your new region,

84
00:03:26,910 --> 00:03:29,580
and from there you can restore a new Redshift cluster

85
00:03:29,580 --> 00:03:32,100
from that copied snapshots.

86
00:03:32,100 --> 00:03:34,860
Now let's talk about how we ingest data into Redshift.

87
00:03:34,860 --> 00:03:37,680
So we have three ways I'm going to describe to you.

88
00:03:37,680 --> 00:03:40,410
The first one is to use Amazon Kinesis Data Firehose.

89
00:03:40,410 --> 00:03:42,510
So we have Firehose that will be receiving data

90
00:03:42,510 --> 00:03:43,830
from different sources,

91
00:03:43,830 --> 00:03:46,290
and then it will send it into Redshift.

92
00:03:46,290 --> 00:03:48,990
And to do so, it will first write the data

93
00:03:48,990 --> 00:03:52,590
into an Amazon S3 bucket, and then Kinesis Data Firehose

94
00:03:52,590 --> 00:03:55,620
will issue automatically an S3 copy command

95
00:03:55,620 --> 00:03:57,887
to load the data into Redshift.

96
00:03:57,887 --> 00:03:59,820
Now, how does this copy come in work?

97
00:03:59,820 --> 00:04:01,200
We can also use it manually.

98
00:04:01,200 --> 00:04:03,510
So you would load data into S3,

99
00:04:03,510 --> 00:04:05,520
and then you would issue a copy command directly

100
00:04:05,520 --> 00:04:07,110
from Redshift to copy data

101
00:04:07,110 --> 00:04:09,300
from an S3 bucket using an IAM role

102
00:04:09,300 --> 00:04:11,940
into your Amazon Redshift cluster.

103
00:04:11,940 --> 00:04:14,010
And as we've seen, there are two ways of doing so.

104
00:04:14,010 --> 00:04:16,079
You might go through the internet

105
00:04:16,079 --> 00:04:18,990
because your S3 buckets are public through the internet.

106
00:04:18,990 --> 00:04:19,822
I mean, they're not public,

107
00:04:19,822 --> 00:04:21,360
but they're connected to the internet.

108
00:04:21,360 --> 00:04:23,280
And so therefore, you're going to have your data flow

109
00:04:23,280 --> 00:04:25,350
through the internet back into your Redshift cluster.

110
00:04:25,350 --> 00:04:28,410
And this is without enhanced VPC routing.

111
00:04:28,410 --> 00:04:30,090
But if you wanted all your network

112
00:04:30,090 --> 00:04:32,910
to remain private into your virtual private cloud,

113
00:04:32,910 --> 00:04:35,550
then you can enable enhance VPC routing

114
00:04:35,550 --> 00:04:39,600
to have all the data flow through the VPC entirely.

115
00:04:39,600 --> 00:04:40,620
And finally, if you wanted

116
00:04:40,620 --> 00:04:43,440
to insert data using the JDBC driver

117
00:04:43,440 --> 00:04:45,570
into the Redshift cluster, you could do so as well.

118
00:04:45,570 --> 00:04:47,700
So for example, if you have an application, an easy

119
00:04:47,700 --> 00:04:49,049
to instance that needs to write data

120
00:04:49,049 --> 00:04:50,790
into your Redshift cluster,

121
00:04:50,790 --> 00:04:53,070
you would need to use this method.

122
00:04:53,070 --> 00:04:56,460
And in that case, it is much better to write large batches

123
00:04:56,460 --> 00:05:00,240
of data into Amazon Redshift instead of one row at a time,

124
00:05:00,240 --> 00:05:03,423
which would be truly inefficient for this type of database.

125
00:05:04,710 --> 00:05:09,330
So a cool feature of Redshift is Redshift Spectrum.

126
00:05:09,330 --> 00:05:12,600
The ideas that you would have data in Amazon S3

127
00:05:12,600 --> 00:05:14,280
and you want to analyze it using Redshift,

128
00:05:14,280 --> 00:05:16,980
but you don't want to load it into Redshift first.

129
00:05:16,980 --> 00:05:18,731
And on top of it, you want

130
00:05:18,731 --> 00:05:20,670
to use a lot more processing power.

131
00:05:20,670 --> 00:05:22,170
So you use Redshift Spectrum

132
00:05:22,170 --> 00:05:24,540
and you must have a Redshift cluster already available

133
00:05:24,540 --> 00:05:25,920
to start the query.

134
00:05:25,920 --> 00:05:27,269
And then once you start the query,

135
00:05:27,269 --> 00:05:29,730
the query will then be submitted to thousand

136
00:05:29,730 --> 00:05:31,200
of Redshift Spectrum nodes

137
00:05:31,200 --> 00:05:34,080
that will perform the query onto your data in S3.

138
00:05:34,080 --> 00:05:35,760
So let's go for an example.

139
00:05:35,760 --> 00:05:37,950
You have your Redshift cluster with a leader node

140
00:05:37,950 --> 00:05:40,650
and a bunch of compute nodes as we've seen,

141
00:05:40,650 --> 00:05:44,160
and the data you want to analyze is in Amazon S3.

142
00:05:44,160 --> 00:05:46,140
So in this case, we're going to run a query

143
00:05:46,140 --> 00:05:48,720
onto our cluster, as we can see.

144
00:05:48,720 --> 00:05:50,670
What we want to do is that the table we want

145
00:05:50,670 --> 00:05:52,710
to query is living in S3.

146
00:05:52,710 --> 00:05:56,250
So we have from S3 dot and whatever you want.

147
00:05:56,250 --> 00:05:58,980
In that case, Spectrum will launch automatically.

148
00:05:58,980 --> 00:06:01,800
And so the query is going to be submitted to thousands

149
00:06:01,800 --> 00:06:04,290
of Redshift Spectrum nodes who are going

150
00:06:04,290 --> 00:06:06,240
to read the data from Amazon S3

151
00:06:06,240 --> 00:06:08,010
and perform some aggregations.

152
00:06:08,010 --> 00:06:09,330
And when they're done, they're going

153
00:06:09,330 --> 00:06:10,560
to send back the results

154
00:06:10,560 --> 00:06:12,750
into your own Amazon Redshift cluster,

155
00:06:12,750 --> 00:06:16,740
and then you will get back into whoever initiated the query.

156
00:06:16,740 --> 00:06:17,573
But with this feature,

157
00:06:17,573 --> 00:06:19,890
we can leverage a lot more processing power

158
00:06:19,890 --> 00:06:22,380
from a shift than the one we have provisioned

159
00:06:22,380 --> 00:06:26,250
into our cluster in the first place,

160
00:06:26,250 --> 00:06:29,193
loading the data from Amazon S3 into Redshift.

161
00:06:39,960 --> 00:06:41,910
So that's it for Amazon Redshift.

162
00:06:41,910 --> 00:06:44,910
I hope you liked it, and I will see you in the next lecture.


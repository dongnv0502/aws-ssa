1
00:00:00,330 --> 00:00:03,420
So now let's talk about AWS Glue.

2
00:00:03,420 --> 00:00:07,830
So Glue is a managed extract, transform and load service,

3
00:00:07,830 --> 00:00:10,680
also called commonly ETL service.

4
00:00:10,680 --> 00:00:11,640
It's very useful

5
00:00:11,640 --> 00:00:15,150
to prepare and transform data for analytics.

6
00:00:15,150 --> 00:00:17,640
So this is a fully serverless service

7
00:00:17,640 --> 00:00:19,770
and you're just going to submit whatever you want

8
00:00:19,770 --> 00:00:23,283
and it will achieve it, for example, say you had data

9
00:00:23,283 --> 00:00:26,190
in an S3 bucket or an Amazon RDS database and you wanted

10
00:00:26,190 --> 00:00:31,110
to load this into a Redshift data warehouse.

11
00:00:31,110 --> 00:00:33,420
So you could extract it using Glue,

12
00:00:33,420 --> 00:00:35,970
then you would transform it if need be to maybe

13
00:00:35,970 --> 00:00:38,130
filter some data or add some columns and so on,

14
00:00:38,130 --> 00:00:40,350
whatever you want, and then you would load

15
00:00:40,350 --> 00:00:44,640
the final output data into a Redshift data warehouse.

16
00:00:44,640 --> 00:00:47,490
So all of this happened from within the Glue ETL service.

17
00:00:47,490 --> 00:00:49,020
You just have to write some code,

18
00:00:49,020 --> 00:00:51,497
launch your ETL job, and off you go.

19
00:00:51,497 --> 00:00:53,340
So that's one example.

20
00:00:53,340 --> 00:00:55,560
Another example that's gonna come up in the exam

21
00:00:55,560 --> 00:00:58,350
is how to convert data into the Parquet format.

22
00:00:58,350 --> 00:00:59,340
So why would we do this?

23
00:00:59,340 --> 00:01:03,480
Because, well, the Parquet format is a columnar data format

24
00:01:03,480 --> 00:01:06,000
and therefore it is much better when in use,

25
00:01:06,000 --> 00:01:08,640
for example, with services like Athena.

26
00:01:08,640 --> 00:01:10,870
So say for example that you are doing

27
00:01:11,790 --> 00:01:14,550
inserts into your S3 buckets

28
00:01:14,550 --> 00:01:17,340
and then these files are in the CSV formats.

29
00:01:17,340 --> 00:01:21,750
Then you would use the Glue ETL service to import the CSV

30
00:01:21,750 --> 00:01:24,840
and convert it into a Parquet format

31
00:01:24,840 --> 00:01:26,400
from within the Glue service.

32
00:01:26,400 --> 00:01:29,790
Then you would send it into an output S3 bucket.

33
00:01:29,790 --> 00:01:32,610
And when in Parquet format, then Amazon Athena

34
00:01:32,610 --> 00:01:36,450
is going to analyze this file in a much better fashion.

35
00:01:36,450 --> 00:01:38,730
So the other thing you can do to automate

36
00:01:38,730 --> 00:01:42,510
this entire process is that anytime a file

37
00:01:42,510 --> 00:01:45,780
is inserted into the S3 bucket, then you can send

38
00:01:45,780 --> 00:01:48,810
events notifications to a Lambda function,

39
00:01:48,810 --> 00:01:51,600
which will then trigger a Glue ETL job.

40
00:01:51,600 --> 00:01:53,460
But you could replace the Lambda function

41
00:01:53,460 --> 00:01:55,200
with EventBridge as well.

42
00:01:55,200 --> 00:01:57,780
This would work as an alternative.

43
00:01:57,780 --> 00:02:00,060
Okay, so there's another feature of Glue

44
00:02:00,060 --> 00:02:03,960
called the Glue Data Catalog, which is to catalog data sets.

45
00:02:03,960 --> 00:02:08,039
So the Glue Data Catalog will run Glue data crawlers

46
00:02:08,039 --> 00:02:11,850
and they will be connected to various data sources

47
00:02:11,850 --> 00:02:16,080
such as Amazon S3, Amazon RDS, Amazon DynamoDB,

48
00:02:16,080 --> 00:02:18,720
or a compatible JDBC database

49
00:02:18,720 --> 00:02:21,180
that you own on premises, for example.

50
00:02:21,180 --> 00:02:25,320
So the Glue Data Catalog is going to crawl these databases

51
00:02:25,320 --> 00:02:28,980
and is going to write all the metadata of your tables,

52
00:02:28,980 --> 00:02:31,650
of your columns, of your data types and so on,

53
00:02:31,650 --> 00:02:33,510
into the Glue Data Catalog.

54
00:02:33,510 --> 00:02:36,720
And so it will have all the databases, the tables,

55
00:02:36,720 --> 00:02:39,240
and the metadata, and that will be leveraged

56
00:02:39,240 --> 00:02:42,780
by the Glue jobs to perform ETF.

57
00:02:42,780 --> 00:02:46,050
Now also, when you use Amazon Athena behind the scenes

58
00:02:46,050 --> 00:02:48,660
to do the data discovery and the schema discovery,

59
00:02:48,660 --> 00:02:51,060
Amazon Athena is going to be leveraging

60
00:02:51,060 --> 00:02:54,030
the AWS Glue Data Catalog.

61
00:02:54,030 --> 00:02:58,590
So will Amazon Retro Spectrum and so will Amazon EMRs.

62
00:02:58,590 --> 00:03:00,900
So as you can see, the Glue Data Catalog service

63
00:03:00,900 --> 00:03:04,710
is central to many other AWS services.

64
00:03:04,710 --> 00:03:07,920
So other features that can appear at the exam on Glue

65
00:03:07,920 --> 00:03:09,720
and that you should know at a high level.

66
00:03:09,720 --> 00:03:12,330
The first one is Glue Job Bookmarks.

67
00:03:12,330 --> 00:03:16,200
And so this is to prevent you from reprocessing all data

68
00:03:16,200 --> 00:03:19,380
in case you run a new ETL job.

69
00:03:19,380 --> 00:03:21,990
So this is very important and it can come up in the exam.

70
00:03:21,990 --> 00:03:23,970
You have Glue Data Brew, which is used

71
00:03:23,970 --> 00:03:27,570
to clean and normalize data using pre-built transformation.

72
00:03:27,570 --> 00:03:29,700
You have Glue Studio, which is a GUI

73
00:03:29,700 --> 00:03:33,420
to create, run, and monitor ETL jobs in Glue.

74
00:03:33,420 --> 00:03:36,540
And then you have Glue Streaming ETL, and it's actually

75
00:03:36,540 --> 00:03:40,170
built on top of Apache Spark Structured Streaming,

76
00:03:40,170 --> 00:03:44,370
and instead of running ETL jobs, as you know, batch jobs

77
00:03:44,370 --> 00:03:46,680
you can run them as streaming jobs.

78
00:03:46,680 --> 00:03:48,930
And so therefore you can read data

79
00:03:48,930 --> 00:03:52,350
using Glue Streaming ETL from Kinesis Data Streams

80
00:03:52,350 --> 00:03:57,350
or Kafka or MSK as we'll see, which is managed Kafka on AWS.

81
00:03:58,530 --> 00:04:00,780
Okay, so that's it for this lecture.

82
00:04:00,780 --> 00:04:03,730
I hope you liked it and I will see you in the next lecture.

